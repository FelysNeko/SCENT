import pytest
import torch
from shared.policies.test_uniform_policy import IndexedList
from shared.samplers.test_sequential_sampler import MockEnv

from rgfn import ExploitationPunishmentHelper, RandomSampler
from rgfn.shared.policies.uniform_policy import UniformPolicy


@pytest.mark.parametrize("n_trajectories", [10])
@pytest.mark.parametrize("n_iterations", [10])
def test__exploitation_punishment_helper__returns_sensible_values(
    n_iterations: int, n_trajectories: int
):
    env = MockEnv()
    policy = UniformPolicy()
    sampler = RandomSampler(env=env, policy=policy, reward=None)
    helper = ExploitationPunishmentHelper()

    for _ in range(n_iterations):
        trajectories = sampler.sample_trajectories(n_trajectories)
        states = trajectories.get_non_last_states_flat()
        forward_action_spaces = trajectories.get_forward_action_spaces_flat()
        weights = helper.compute_weights(
            states, forward_action_spaces, action_space_size=env.max_component + 1
        )
        assert torch.allclose(torch.sum(weights, dim=-1), torch.ones(len(states)))


@pytest.mark.parametrize("n_states", [10])
def test__exploitation_punishment_helper__single_actions(n_states: int):
    helper = ExploitationPunishmentHelper()

    action_spaces = [IndexedList([0]) for _ in range(n_states)]
    states = [0] * n_states
    weights = helper.compute_weights(states, action_spaces, action_space_size=1)
    assert torch.allclose(weights, torch.ones(n_states))


@pytest.mark.parametrize("n_trajectories", [10])
@pytest.mark.parametrize("n_iterations", [10])
def test__exploitation_punishment_helper__returns_uniform(n_iterations: int, n_trajectories: int):
    env = MockEnv()
    policy = UniformPolicy()
    sampler = RandomSampler(env=env, policy=policy, reward=None)
    helper = ExploitationPunishmentHelper(initial_temperature=0.0)

    for _ in range(n_iterations):
        trajectories = sampler.sample_trajectories(n_trajectories)
        states = trajectories.get_non_last_states_flat()
        forward_action_spaces = trajectories.get_forward_action_spaces_flat()
        weights = helper.compute_weights(
            states, forward_action_spaces, action_space_size=env.max_component + 1
        )
        for state_weight in weights:
            non_zero = state_weight[state_weight != 0]
            assert torch.all(non_zero == non_zero[0])

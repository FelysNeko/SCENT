import gin_config
import rgfn

user_root_dir = 'experiments'
include 'configs/loggers/wandb.gin'
include 'configs/samplers/random.gin'
include 'configs/rewards/exponential.gin'
include 'configs/policies/reaction.gin'
include 'configs/objectives/trajectory_balance.gin'
include 'configs/replay_buffers/reward_prioritized.gin'

run_dir = @run_dir/get_str()
run_dir/get_str.format = '{}/{}'
run_dir/get_str.values = [%user_root_dir, %run_name]

TrajectoryBalanceOptimizer.cls_name = 'Adam'
TrajectoryBalanceOptimizer.lr = 0.001
TrajectoryBalanceOptimizer.logZ_multiplier = 100.0

train_metrics = [@StandardGFNMetrics(), @NumScaffoldsFound(), @UniqueMolecules(), @ProxyCalls(), @FractionEarlyTerminate(), @QED()]

ProxyCalls.proxy = %train_proxy
NumScaffoldsFound.proxy_value_threshold_list = [5, 6, 7, 8]
NumScaffoldsFound.proxy_component_name = None
UniqueMolecules.dump_every_n = 500
UniqueMolecules.run_dir = %run_dir

Reward.beta = 8
Trainer.run_dir = %run_dir
Trainer.train_forward_sampler = %train_forward_sampler
Trainer.train_replay_buffer = %train_replay_buffer
Trainer.train_metrics = %train_metrics
Trainer.objective = %objective
Trainer.optimizer = @TrajectoryBalanceOptimizer()
Trainer.lr_scheduler = None
Trainer.n_iterations = 5005
Trainer.train_forward_n_trajectories = 100
Trainer.train_backward_n_trajectories = 0
Trainer.train_replay_n_trajectories = 20
Trainer.valid_n_trajectories = 100
Trainer.valid_every_n_iterations = 500
Trainer.logger = %logger
Trainer.device = 'auto'

WandbLogger.mode = 'online'

ExploratoryPolicy.first_policy_weight = 0.95
TrajectoryBalanceObjective.z_dim = 16

linear_output = False
ReactionForwardPolicy.linear_output = %linear_output
ReactionBackwardPolicy.linear_output = %linear_output

FragmentFingerprintEmbedding.data_factory = %data_factory
action_embedding_fn/gin.singleton.constructor = @FragmentOneHotEmbedding
FragmentFingerprintEmbedding.random_linear_compression = False
FragmentFingerprintEmbedding.fingerprint_list = ['maccs']
FragmentFingerprintEmbedding.one_hot_weight = 1.0

import gin_config
import rgfn

user_root_dir = 'experiments'
include 'configs/loggers/wandb.gin'
include 'configs/samplers/random.gin'
include 'configs/rewards/exponential.gin'
include 'configs/policies/reaction.gin'
include 'configs/policies/exploration/uniform.gin'
include 'configs/policies/action_embeddings/one_hot.gin'
include 'configs/objectives/trajectory_balance.gin'
include 'configs/replay_buffers/reward_prioritized.gin'

run_dir = @run_dir/get_str()
run_dir/get_str.format = '{}/{}'
run_dir/get_str.values = [%user_root_dir, %run_name]

Reward.beta = 8
TrajectoryBalanceOptimizer.cls_name = 'Adam'
TrajectoryBalanceOptimizer.lr = 0.001
TrajectoryBalanceOptimizer.logZ_multiplier = 100.0

train_metrics = [@StandardGFNMetrics(), @NumScaffoldsFound(), @UniqueMolecules(), @AllMolecules(), @FractionEarlyTerminate(), @QED(), @TanimotoSimilarityModes()]
evaluation_step = 500

NumScaffoldsFound.proxy_value_threshold_list = [5, 6, 7, 8]
NumScaffoldsFound.proxy_component_name = None
UniqueMolecules.dump_every_n = %evaluation_step
UniqueMolecules.run_dir = %run_dir
AllMolecules.dump_every_n = %evaluation_step
AllMolecules.run_dir = %run_dir
TanimotoSimilarityModes.run_dir = %run_dir
TanimotoSimilarityModes.proxy = %train_proxy
TanimotoSimilarityModes.compute_every_n = %evaluation_step
TanimotoSimilarityModes.similarity_threshold = 0.3
TanimotoSimilarityModes.max_modes = 1000

Trainer.run_dir = %run_dir
Trainer.train_forward_sampler = %train_forward_sampler
Trainer.train_replay_buffer = %train_replay_buffer
Trainer.train_metrics = %train_metrics
Trainer.objective = %objective
Trainer.optimizer = @TrajectoryBalanceOptimizer()
Trainer.lr_scheduler = None
Trainer.n_iterations = 5005
Trainer.train_forward_n_trajectories = 100
Trainer.train_backward_n_trajectories = 0
Trainer.train_replay_n_trajectories = 20
Trainer.logger = %logger
Trainer.device = 'auto'
Trainer.trajectory_filter = @RGFNTrajectoryFilter()
